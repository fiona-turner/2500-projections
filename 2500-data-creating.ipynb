{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import linear_model\n",
    "import pandas as pd\n",
    "import GPy\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import mpl_toolkits\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import netCDF4 as nc\n",
    "import glob\n",
    "import scipy.io\n",
    "from cycler import cycler\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the MICI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2300_26 = pd.read_excel('./Data/41586_2021_3427_MOESM6_ESM.xlsx', 'RCP2.6 sea level equivalent (m)', engine='openpyxl')\n",
    "df2300_26 = df2300_26.iloc[:,:110]\n",
    "\n",
    "df2300_45 = pd.read_excel('./Data/41586_2021_3427_MOESM6_ESM.xlsx', 'RCP4.5 sea level equivalent (m)', engine='openpyxl')\n",
    "df2300_45 = df2300_45.iloc[:,:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data so baseline is 2020, as all of our data will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2300_26.iloc[:,1:] = df2300_26.iloc[:,1:] - df2300_26.iloc[20,1:]\n",
    "df2300_45.iloc[:,1:] = df2300_45.iloc[:,1:] - df2300_45.iloc[20,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the last 50 years of simulations, and try fitting a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.linspace(2301, 2500, 200)\n",
    "coef_26 = np.zeros((df2300_26.shape[1]-1,2))\n",
    "\n",
    "for i in np.arange(df2300_26.shape[1])[1:]:\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(df2300_26.iloc[250:301,0].to_frame(), df2300_26.iloc[250:301,i].to_frame())\n",
    "    coef_26[i-1,0] = regr.coef_\n",
    "    coef_26[i-1,1] = regr.intercept_\n",
    "\n",
    "coef_45 = np.zeros((df2300_45.shape[1]-1,2))\n",
    "\n",
    "for i in np.arange(df2300_45.shape[1])[1:]:\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(df2300_45.iloc[250:301,0].to_frame(), df2300_45.iloc[250:301,i].to_frame())\n",
    "    coef_45[i-1,0] = regr.coef_\n",
    "    coef_45[i-1,1] = regr.intercept_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Meinshausen et al (2020), CO2 concentrations will not rise linearly after 2300 but will instead decrease slightly. A linear prediction therefore may be unreasonable to calculate. Instead we consider two extremes, a linear increase or a near flat increase in sea level rise, calculated as 10% of the linear model gradient. We sample inbetween each of these extremes using a Uniform distribution, randomly assigning the gradients to each simulation and using the 2300 prediction as the intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "years = np.linspace(2301, 2500, 200)\n",
    "years = np.reshape(years, (200,1))\n",
    "years2 = np.linspace(1, 200, 200)\n",
    "pp_26 = np.zeros((200,coef_26.shape[0]))\n",
    "grad_26 = np.zeros(coef_26.shape[0])\n",
    "for i in np.arange(coef_26.shape[0]):\n",
    "    grad_26[i] = np.random.uniform(0.1*coef_26[i,0], coef_26[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_26[j,i] = grad_26[i]*years2[j] + df2300_26.iloc[300,i+1]\n",
    "    \n",
    "pp_45 = np.zeros((200,coef_45.shape[0]))\n",
    "grad_45 = np.zeros(coef_45.shape[0])\n",
    "for i in np.arange(coef_45.shape[0]):\n",
    "    grad_45[i] = np.random.uniform(0.1*coef_45[i,0], coef_45[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_45[j,i] = grad_45[i]*years2[j] + df2300_45.iloc[300,i+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeConto_26_2500 = np.concatenate((df2300_26.iloc[:,1:],pp_26))\n",
    "DeConto_45_2500 = np.concatenate((df2300_45.iloc[0:301,1:],pp_45))\n",
    "DeConto_years = np.concatenate((df2300_26['Year'],np.concatenate(years)))\n",
    "\n",
    "DeConto_26_csv = np.concatenate((np.reshape(DeConto_years, (501,1)), DeConto_26_2500),axis=1)\n",
    "DeConto_45_csv = np.concatenate((np.reshape(DeConto_years, (501,1)), DeConto_45_2500),axis=1)\n",
    "\n",
    "#np.savetxt('./Created_Data/DeConto_2500_RCP26.csv', DeConto_26_csv[20:,:], delimiter=',')\n",
    "#np.savetxt('./Created_Data/DeConto_2500_RCP45.csv', DeConto_45_csv[20:,:], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.array([0.05, 0.17, 0.5, 0.83, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = np.array([0.   , 0.001, 0.005, 0.01 , 0.02 , 0.03 , 0.04 , 0.05 ,\n",
    "                   0.06 , 0.07 , 0.08 , 0.09 , 0.1  , 0.11 , 0.12 , 0.13 ,\n",
    "                   0.14 , 0.15 , 0.16 , 0.167, 0.17 , 0.18 , 0.19 , 0.2  ,\n",
    "                   0.21 , 0.22 , 0.23 , 0.24 , 0.25 , 0.26 , 0.27 , 0.28 ,\n",
    "                   0.29 , 0.3  , 0.31 , 0.32 , 0.33 , 0.34 , 0.35 , 0.36 ,\n",
    "                   0.37 , 0.38 , 0.39 , 0.4  , 0.41 , 0.42 , 0.43 , 0.44 ,\n",
    "                   0.45 , 0.46 , 0.47 , 0.48 , 0.49 , 0.5  , 0.51 , 0.52 ,\n",
    "                   0.53 , 0.54 , 0.55 , 0.56 , 0.57 , 0.58 , 0.59 , 0.6  ,\n",
    "                   0.61 , 0.62 , 0.63 , 0.64 , 0.65 , 0.66 , 0.67 , 0.68 ,\n",
    "                   0.69 , 0.7  , 0.71 , 0.72 , 0.73 , 0.74 , 0.75 , 0.76 ,\n",
    "                   0.77 , 0.78 , 0.79 , 0.8  , 0.81 , 0.82 , 0.83 , 0.833,\n",
    "                   0.84 , 0.85 , 0.86 , 0.87 , 0.88 , 0.89 , 0.9  , 0.91 ,\n",
    "                   0.92 , 0.93 , 0.94 , 0.95 , 0.96 , 0.97 , 0.98 , 0.99 ,\n",
    "                   0.995, 0.999, 1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have the Bulthuis et al data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bult_26_m1 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m1/RCP26/response.mat', squeeze_me = True)\n",
    "bult_45_m1 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m1/RCP45/response.mat', squeeze_me = True)\n",
    "\n",
    "bult_26_m2 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m2/RCP26/response.mat', squeeze_me = True)\n",
    "bult_45_m2 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m2/RCP45/response.mat', squeeze_me = True)\n",
    "\n",
    "bult_26_m2t = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m2Tsai/RCP26/response.mat', squeeze_me = True)\n",
    "bult_45_m2t = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m2Tsai/RCP45/response.mat', squeeze_me = True)\n",
    "\n",
    "bult_26_m3 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m3/RCP26/response.mat', squeeze_me = True)\n",
    "bult_45_m3 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m3/RCP45/response.mat', squeeze_me = True)\n",
    "\n",
    "bult_26_m5 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m5/RCP26/response.mat', squeeze_me = True)\n",
    "bult_45_m5 = scipy.io.loadmat('./Data/Data_Bulthuis_ensemble/m5/RCP45/response.mat', squeeze_me = True)\n",
    "\n",
    "bult_26 = np.vstack((bult_26_m1['data'], bult_26_m2['data'], bult_26_m2t['data'], bult_26_m3['data'], bult_26_m5['data']))\n",
    "bult_45 = np.vstack((bult_45_m1['data'], bult_45_m2['data'], bult_45_m2t['data'], bult_45_m3['data'], bult_45_m5['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data so baseline is 2020, then interpolate to annual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bult_26 = bult_26 - bult_26[:,5:6]\n",
    "bult_45 = bult_45 - bult_45[:,5:6]\n",
    "\n",
    "\n",
    "annual_bult_26 = np.zeros((DeConto_years.shape[0],bult_26.shape[0]))\n",
    "for i in np.arange(bult_26.shape[0]):\n",
    "    f126 = interp1d(bult_26_m1['time'][0:102], bult_26[i,0:102], fill_value=\"extrapolate\")\n",
    "    annual_bult_26[:,i] = f126(DeConto_years)\n",
    "\n",
    "annual_bult_45 = np.zeros((DeConto_years.shape[0],bult_45.shape[0]))\n",
    "for i in np.arange(bult_45.shape[0]):\n",
    "    f245 = interp1d(bult_45_m1['time'][0:102], bult_45[i,0:102], fill_value=\"extrapolate\")\n",
    "    annual_bult_45[:,i] = f245(DeConto_years)\n",
    "\n",
    "bult_26_csv = np.concatenate((np.reshape(DeConto_years, (501,1)), annual_bult_26),axis=1)\n",
    "bult_45_csv = np.concatenate((np.reshape(DeConto_years, (501,1)), annual_bult_45),axis=1)\n",
    "\n",
    "#np.savetxt('./Created_Data/Bulthuis_2500_RCP26.csv', bult_26_csv[20:,:], delimiter=',')\n",
    "#np.savetxt('./Created_Data/Bulthuis_2500_RCP45.csv', bult_45_csv[20:,:], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the Lowry et al data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lowry_26 = pd.read_csv('./Data/Lowry_et_al/emulator_runs/emulator_runs_rcp26.csv')\n",
    "\n",
    "Lowry_45 = pd.read_csv('./Data/Lowry_et_al/emulator_runs/emulator_runs_rcp45.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data so baseline is 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lowry_26.iloc[:,2:] = Lowry_26.iloc[:,2:] - Lowry_26.iloc[50,2:]\n",
    "Lowry_45.iloc[:,2:] = Lowry_45.iloc[:,2:] - Lowry_45.iloc[50,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extrapolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_Lowry_26 = np.zeros((Lowry_26.shape[1]-2,2))\n",
    "\n",
    "for i in np.arange(Lowry_26.shape[1])[2:]:\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(Lowry_26.iloc[280:331,0].to_frame(), Lowry_26.iloc[280:331,i].to_frame())\n",
    "    coef_Lowry_26[i-2,0] = regr.coef_\n",
    "    coef_Lowry_26[i-2,1] = regr.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "years2 = np.linspace(1, 200, 200)\n",
    "np.random.seed(0)\n",
    "pp_Lowry_26 = np.zeros((200,coef_Lowry_26.shape[0]))\n",
    "grad_Lowry_26 = np.zeros(coef_Lowry_26.shape[0])\n",
    "for i in np.arange(coef_Lowry_26.shape[0]):\n",
    "    grad_Lowry_26[i] = np.random.uniform(0.1*coef_Lowry_26[i,0], coef_Lowry_26[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_Lowry_26[j,i] = grad_Lowry_26[i]*years2[j] + Lowry_26.iloc[330,i+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_Lowry_45 = np.zeros((Lowry_45.shape[1]-2,2))\n",
    "\n",
    "for i in np.arange(Lowry_45.shape[1])[2:]:\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(Lowry_45.iloc[280:331,0].to_frame(), Lowry_45.iloc[280:331,i].to_frame())\n",
    "    coef_Lowry_45[i-2,0] = regr.coef_\n",
    "    coef_Lowry_45[i-2,1] = regr.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "pp_Lowry_45 = np.zeros((200,coef_Lowry_45.shape[0]))\n",
    "grad_Lowry_45 = np.zeros(coef_Lowry_45.shape[0])\n",
    "for i in np.arange(coef_Lowry_45.shape[0]):\n",
    "    grad_Lowry_45[i] = np.random.uniform(0.1*coef_Lowry_45[i,0], coef_Lowry_45[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_Lowry_45[j,i] = grad_Lowry_45[i]*years2[j] + Lowry_45.iloc[330,i+2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "years2 = np.reshape(years2, (200,1))\n",
    "Lowry_26_2500 = np.concatenate((Lowry_26.iloc[:,2:],pp_Lowry_26))\n",
    "Lowry_45_2500 = np.concatenate((Lowry_45.iloc[:,2:],pp_Lowry_45))\n",
    "Lowry_years = np.concatenate((Lowry_26['year'],np.concatenate(years2+2300)))\n",
    "\n",
    "Lowry_26_csv = np.concatenate((np.reshape(Lowry_years, (531,1)), Lowry_26_2500),axis=1)\n",
    "Lowry_45_csv = np.concatenate((np.reshape(Lowry_years, (531,1)), Lowry_45_2500),axis=1)\n",
    "\n",
    "#np.savetxt('./Created_Data/Lowry_2500_RCP26.csv', Lowry_26_csv[50:,:], delimiter=',')\n",
    "#np.savetxt('./Created_Data/Lowry_2500_RCP45.csv', Lowry_45_csv[50:,:], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the LARMIP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = './Data/fortamsin_221025/icesheets-ipccar6-larmipicesheet-ssp126_TOT_globalsl.nc'\n",
    "ds_LAR126 = nc.Dataset(fn)\n",
    "slc_126 = ds_LAR126['sea_level_change'][:,:,:]\n",
    "slc_126 = np.squeeze(slc_126)\n",
    "slc_126 = slc_126.transpose()\n",
    "slc_126 = slc_126 - slc_126[0,:]\n",
    "\n",
    "years_TOT = ds_LAR126['years'][:]\n",
    "years_TOT\n",
    "\n",
    "fn = './Data/fortamsin_221025/icesheets-ipccar6-larmipicesheet-ssp245_TOT_globalsl.nc'\n",
    "ds_LAR245 = nc.Dataset(fn)\n",
    "slc_245 = ds_LAR245['sea_level_change'][:,:,:]\n",
    "slc_245 = np.squeeze(slc_245)\n",
    "slc_245 = slc_245.transpose()\n",
    "slc_245 = slc_245 - slc_245[0,:]\n",
    "\n",
    "q = [7, 19, 53, 87, 99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to interpolate to annual data, and then extrapolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearsnew = np.linspace(2020, 2300, num=281, endpoint=True)\n",
    "yearsnew = np.reshape(yearsnew, (281,))\n",
    "\n",
    "np.random.seed(0)\n",
    "annualLAR126 = np.zeros((yearsnew.shape[0],slc_126.shape[1]))\n",
    "LAR126_2500 = np.zeros((years.shape[0],slc_126.shape[1]))\n",
    "\n",
    "for i in np.arange(slc_126.shape[1]):\n",
    "    f126 = interp1d(years_TOT, slc_126[:,i], fill_value=\"extrapolate\")\n",
    "    annualLAR126[:,i] = f126(yearsnew)\n",
    "\n",
    "coef_LAR_126 = np.zeros((annualLAR126.shape[1],2))\n",
    "\n",
    "for i in np.arange(annualLAR126.shape[1]):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(yearsnew[231:281].reshape((50,1)), annualLAR126[231:281,i].reshape((50,1)))\n",
    "    coef_LAR_126[i,0] = regr.coef_\n",
    "    coef_LAR_126[i,1] = regr.intercept_\n",
    "\n",
    "pp_LAR_126 = np.zeros((200,coef_LAR_126.shape[0]))\n",
    "grad_LAR_126 = np.zeros(coef_LAR_126.shape[0])\n",
    "for i in np.arange(coef_LAR_126.shape[0]):\n",
    "    grad_LAR_126[i] = np.random.uniform(0.1*coef_LAR_126[i,0], coef_LAR_126[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_LAR_126[j,i] = grad_LAR_126[i]*(j+1) + annualLAR126[280,i]       \n",
    "\n",
    "        \n",
    "annualLAR245 = np.zeros((yearsnew.shape[0],slc_245.shape[1]))\n",
    "LAR245_2500 = np.zeros((years.shape[0],slc_245.shape[1]))\n",
    "\n",
    "for i in np.arange(slc_245.shape[1]):\n",
    "    f245 = interp1d(years_TOT, slc_245[:,i], fill_value=\"extrapolate\")\n",
    "    annualLAR245[:,i] = f245(yearsnew)\n",
    "\n",
    "coef_LAR_245 = np.zeros((annualLAR245.shape[1],2))\n",
    "\n",
    "for i in np.arange(annualLAR245.shape[1]):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(yearsnew[231:281].reshape((50,1)), annualLAR245[231:281,i].reshape((50,1)))\n",
    "    coef_LAR_245[i,0] = regr.coef_\n",
    "    coef_LAR_245[i,1] = regr.intercept_\n",
    "\n",
    "pp_LAR_245 = np.zeros((200,coef_LAR_245.shape[0]))\n",
    "grad_LAR_245 = np.zeros(coef_LAR_245.shape[0])\n",
    "for i in np.arange(coef_LAR_245.shape[0]):\n",
    "    grad_LAR_245[i] = np.random.uniform(0.1*coef_LAR_245[i,0], coef_LAR_245[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_LAR_245[j,i] = grad_LAR_245[i]*(j+1) + annualLAR245[280,i]   \n",
    "\n",
    "\n",
    "LAR_years = np.append(yearsnew,years)\n",
    "LAR_126 = np.concatenate((annualLAR126,pp_LAR_126))/1000\n",
    "LAR_245 = np.concatenate((annualLAR245,pp_LAR_245))/1000\n",
    "\n",
    "LAR_126_csv = np.concatenate((np.reshape(LAR_years, (481,1)), LAR_126),axis=1)\n",
    "LAR_245_csv = np.concatenate((np.reshape(LAR_years, (481,1)), LAR_245),axis=1)\n",
    "\n",
    "#np.savetxt('./Created_Data/LARMIP_2500_SSP126.csv', LAR_126_csv, delimiter=',')\n",
    "#np.savetxt('./Created_Data/LARMIP_2500_SSP245.csv', LAR_245_csv, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on in Greenland?\n",
    "\n",
    "Look at Aschwanden et al first.\n",
    "\n",
    "Load data, which is stored in kg. Convert to SLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files45 = glob.glob(\"./Data/resource_map_doi_10_18739_A2222R58F/data/*rcp_45*\")\n",
    "files26 = glob.glob(\"./Data/resource_map_doi_10_18739_A2222R58F/data/*rcp_26*\")\n",
    "\n",
    "kg45 = np.zeros((1000, len(files45)))\n",
    "\n",
    "for i in np.arange(len(files45)):\n",
    "    fn = files45[i]\n",
    "    ds = nc.Dataset(fn)\n",
    "    kg45[:,i] = ds['limnsw'][:]\n",
    "    \n",
    "time = ds['time'][:]\n",
    "time = np.reshape(time, (1000,1))\n",
    "\n",
    "kg45 = np.append(np.rint(time/31536000 + 2008), kg45, axis=1)\n",
    "\n",
    "\n",
    "kg26 = np.zeros((1000, len(files26)))\n",
    "\n",
    "for i in np.arange(len(files26)):\n",
    "    fn = files26[i]\n",
    "    ds = nc.Dataset(fn)\n",
    "    kg26[:,i] = ds['limnsw'][:]\n",
    "    \n",
    "kg26 = np.append(np.rint(time/31536000 + 2008), kg26, axis=1)\n",
    "\n",
    "grisyears = np.array(np.linspace(2008, 3007, num=1000))\n",
    "grisyears = np.reshape(grisyears, (1000,1))\n",
    "\n",
    "SLE26 = -1*(kg26[:,1:] - kg26[0,1:])/(365*(10**13))\n",
    "SLE26 = SLE26[:,::-1]\n",
    "SLE26 = np.append(grisyears, SLE26, axis=1)\n",
    "\n",
    "SLE45 = -1*(kg45[:,1:] - kg45[0,1:])/(365*(10**13))\n",
    "SLE45 = SLE45[:,::-1]\n",
    "SLE45 = np.append(grisyears, SLE45, axis=1)\n",
    "\n",
    "SLE26[:, 1:] = (SLE26[:,1:] - SLE26[12,1:])/100\n",
    "SLE45[:, 1:] = (SLE45[:,1:] - SLE45[12,1:])/100\n",
    "\n",
    "#np.savetxt('./Created_Data/Aschwanden_2500_RCP26.csv', SLE26[12:493,:], delimiter=',')\n",
    "#np.savetxt('./Created_Data/Aschwanden_2500_RCP45.csv', SLE45[12:493,:], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AR6 Greenland data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    description: Global SLR contribution from GIS according to FittedISMIP icesheets workflow\n",
      "    history: Created Fri Jun 25 15:03:59 2021\n",
      "    source: FACTS: icesheets-FittedISMIP-icesheets-ssp126\n",
      "    scenario: ssp126\n",
      "    baseyear: 2005\n",
      "    dimensions(sizes): years(29), samples(20000), locations(1)\n",
      "    variables(dimensions): int32 years(years), int64 samples(samples), int64 locations(locations), float32 lat(locations), float32 lon(locations), int16 sea_level_change(samples, years, locations)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "fn = './Data/AR6_GSAT,_Greenland_and_Antarctic_data/AR6_FittedISMIP_GIS_LARMIP_AIS/icesheets-FittedISMIP-icesheets-ssp126_GIS_globalsl.nc'\n",
    "ds_26 = nc.Dataset(fn)\n",
    "print(ds_26)\n",
    "gris26 = ds_26['sea_level_change'][:,:,:]\n",
    "gris26 = np.squeeze(gris26)\n",
    "gris26 = gris26.transpose()\n",
    "\n",
    "years_ar6 = ds_26['years'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    description: Global SLR contribution from GIS according to FittedISMIP icesheets workflow\n",
      "    history: Created Fri Jun 25 15:04:06 2021\n",
      "    source: FACTS: icesheets-FittedISMIP-icesheets-ssp245\n",
      "    scenario: ssp245\n",
      "    baseyear: 2005\n",
      "    dimensions(sizes): years(29), samples(20000), locations(1)\n",
      "    variables(dimensions): int32 years(years), int64 samples(samples), int64 locations(locations), float32 lat(locations), float32 lon(locations), int16 sea_level_change(samples, years, locations)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "fn = './Data/AR6_GSAT,_Greenland_and_Antarctic_data/AR6_FittedISMIP_GIS_LARMIP_AIS/icesheets-FittedISMIP-icesheets-ssp245_GIS_globalsl.nc'\n",
    "ds_45 = nc.Dataset(fn)\n",
    "print(ds_45)\n",
    "gris45 = ds_45['sea_level_change'][:,:,:]\n",
    "gris45 = np.squeeze(gris45)\n",
    "gris45 = gris45.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change baseline to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gris26 = gris26[:,:] - gris26[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gris45 = gris45[:,:] - gris45[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate into annual data, then extrapolate final fifty years using uniform choice of gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "xnew_ar6 = np.linspace(2301, 2500, num=200, endpoint=True)\n",
    "yearsnew_ar6 = np.linspace(2020, 2300, num=281, endpoint=True)\n",
    "annual26 = np.zeros((yearsnew_ar6.shape[0],gris26.shape[1]))\n",
    "gris26_2500 = np.zeros((xnew_ar6.shape[0],gris26.shape[1]))\n",
    "\n",
    "for i in np.arange(gris26.shape[1]):\n",
    "    f26 = interp1d(years_ar6, gris26[:,i], fill_value=\"extrapolate\")\n",
    "    annual26[:,i] = f26(yearsnew_ar6)\n",
    "\n",
    "coef_GrIS_26 = np.zeros((annual26.shape[1],2))\n",
    "\n",
    "for i in np.arange(annual26.shape[1]):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(yearsnew_ar6[81:131].reshape((50,1)), annual26[81:131,i].reshape((50,1)))\n",
    "    coef_GrIS_26[i,0] = regr.coef_\n",
    "    coef_GrIS_26[i,1] = regr.intercept_\n",
    "\n",
    "pp_GrIS_26 = np.zeros((xnew_ar6.shape[0],coef_GrIS_26.shape[0])) \n",
    "grad_GrIS_26 = np.zeros(coef_GrIS_26.shape[0])\n",
    "for i in np.arange(coef_GrIS_26.shape[0]):\n",
    "    grad_GrIS_26[i] = np.random.uniform(0.1*coef_GrIS_26[i,0], coef_GrIS_26[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_GrIS_26[j,i] = grad_GrIS_26[i]*(j+1) + annual26[280,i]   \n",
    "\n",
    "annual45 = np.zeros((yearsnew_ar6.shape[0],gris45.shape[1]))\n",
    "gris45_2500 = np.zeros((xnew_ar6.shape[0],gris45.shape[1]))\n",
    "\n",
    "for i in np.arange(gris45.shape[1]):\n",
    "    f45 = interp1d(years_ar6, gris45[:,i], fill_value=\"extrapolate\")\n",
    "    annual45[:,i] = f45(yearsnew_ar6)\n",
    "\n",
    "coef_GrIS_45 = np.zeros((annual45.shape[1],2))\n",
    "\n",
    "for i in np.arange(annual45.shape[1]):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(yearsnew_ar6[81:131].reshape((50,1)), annual45[81:131,i].reshape((50,1)))\n",
    "    coef_GrIS_45[i,0] = regr.coef_\n",
    "    coef_GrIS_45[i,1] = regr.intercept_\n",
    "\n",
    "pp_GrIS_45 = np.zeros((xnew_ar6.shape[0],coef_GrIS_45.shape[0])) \n",
    "grad_GrIS_45 = np.zeros(coef_GrIS_45.shape[0])\n",
    "for i in np.arange(coef_GrIS_45.shape[0]):\n",
    "    grad_GrIS_45[i] = np.random.uniform(0.1*coef_GrIS_45[i,0], coef_GrIS_45[i,0])\n",
    "    for j in np.arange(200):\n",
    "        pp_GrIS_45[j,i] = grad_GrIS_45[i]*(j+1) + annual45[280,i]   \n",
    "\n",
    "\n",
    "gris_years = np.append(yearsnew_ar6,xnew_ar6)\n",
    "gris_26 = np.concatenate((annual26,pp_GrIS_26))\n",
    "gris_45 = np.concatenate((annual45,pp_GrIS_45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gris_126_csv = np.concatenate((np.reshape(gris_years, (481,1)), gris_26/1000),axis=1)\n",
    "gris_245_csv = np.concatenate((np.reshape(gris_years, (481,1)), gris_45/1000),axis=1)\n",
    "#np.savetxt('./Created_Data/GrIS_AR6_2500_SSP126.csv', gris_126_csv, delimiter=',')\n",
    "#np.savetxt('./Created_Data/GrIS_AR6_2500_SSP245.csv', gris_245_csv, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with the glaciers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data=[0.   , 0.001, 0.005, 0.01 , 0.02 , 0.03 , 0.04 , 0.05 ,\n",
       "                   0.06 , 0.07 , 0.08 , 0.09 , 0.1  , 0.11 , 0.12 , 0.13 ,\n",
       "                   0.14 , 0.15 , 0.16 , 0.167, 0.17 , 0.18 , 0.19 , 0.2  ,\n",
       "                   0.21 , 0.22 , 0.23 , 0.24 , 0.25 , 0.26 , 0.27 , 0.28 ,\n",
       "                   0.29 , 0.3  , 0.31 , 0.32 , 0.33 , 0.34 , 0.35 , 0.36 ,\n",
       "                   0.37 , 0.38 , 0.39 , 0.4  , 0.41 , 0.42 , 0.43 , 0.44 ,\n",
       "                   0.45 , 0.46 , 0.47 , 0.48 , 0.49 , 0.5  , 0.51 , 0.52 ,\n",
       "                   0.53 , 0.54 , 0.55 , 0.56 , 0.57 , 0.58 , 0.59 , 0.6  ,\n",
       "                   0.61 , 0.62 , 0.63 , 0.64 , 0.65 , 0.66 , 0.67 , 0.68 ,\n",
       "                   0.69 , 0.7  , 0.71 , 0.72 , 0.73 , 0.74 , 0.75 , 0.76 ,\n",
       "                   0.77 , 0.78 , 0.79 , 0.8  , 0.81 , 0.82 , 0.83 , 0.833,\n",
       "                   0.84 , 0.85 , 0.86 , 0.87 , 0.88 , 0.89 , 0.9  , 0.91 ,\n",
       "                   0.92 , 0.93 , 0.94 , 0.95 , 0.96 , 0.97 , 0.98 , 0.99 ,\n",
       "                   0.995, 0.999, 1.   ],\n",
       "             mask=False,\n",
       "       fill_value=1e+20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn126 = './Data/glaciers-ar5-glaciersgmip2/glaciers-ar5-glaciersgmip2-ssp126_globalsl.nc'\n",
    "ds126 = nc.Dataset(fn126)\n",
    "\n",
    "fn245 = './Data/glaciers-ar5-glaciersgmip2/glaciers-ar5-glaciersgmip2-ssp245_globalsl.nc'\n",
    "ds245 = nc.Dataset(fn245)\n",
    "\n",
    "years_glac = ds126['years'][:]\n",
    "years_glac\n",
    "\n",
    "quant = ds126['quantiles'][:]\n",
    "quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and set baseline to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [7, 19, 53, 87, 99]\n",
    "glac126 = ds126['sea_level_change'][:]\n",
    "glac126 = np.squeeze(glac126)\n",
    "glac126 = glac126.T\n",
    "glac126 = glac126 - glac126[0,q[2]]\n",
    "\n",
    "glac245 = ds245['sea_level_change'][:]\n",
    "glac245 = np.squeeze(glac245)\n",
    "glac245 = glac245.T\n",
    "glac245 = glac245 - glac245[0,q[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate to annual data, then extrapolate to 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual126 = np.zeros((yearsnew_ar6.shape[0],glac126.shape[1]))\n",
    "glac126_2500 = np.zeros((xnew_ar6.shape[0],glac126.shape[1]))\n",
    "\n",
    "for i in np.arange(glac126.shape[1]):\n",
    "    f126 = interp1d(years_glac, glac126[:,i], fill_value=\"extrapolate\")\n",
    "    annual126[:,i] = f126(yearsnew_ar6)\n",
    "    glac126_2500[:,i] = f126(xnew_ar6)\n",
    "\n",
    "annual245 = np.zeros((yearsnew.shape[0],glac245.shape[1]))\n",
    "glac245_2500 = np.zeros((xnew_ar6.shape[0],glac245.shape[1]))\n",
    "\n",
    "for i in np.arange(glac245.shape[1]):\n",
    "    f245 = interp1d(years_glac, glac245[:,i], fill_value=\"extrapolate\")\n",
    "    annual245[:,i] = f245(yearsnew_ar6)\n",
    "    glac245_2500[:,i] = f245(xnew_ar6)\n",
    "\n",
    "yearsnew = np.reshape(yearsnew, (281,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a cap on glaciers contribution to sea level. This is set as the max value in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(annual126.shape[0]):\n",
    "    for j in np.arange(annual126.shape[1]):\n",
    "        if annual126[i,j] > 294: annual126[i,j] = 294\n",
    "glacannual126 = np.hstack([yearsnew,annual126/10])\n",
    "\n",
    "for i in np.arange(annual245.shape[0]):\n",
    "    for j in np.arange(annual245.shape[1]):\n",
    "        if annual245[i,j] > 294: annual245[i,j] = 294\n",
    "glacannual245 = np.hstack([yearsnew,annual245/10])\n",
    "\n",
    "for i in np.arange(glac126_2500.shape[0]):\n",
    "    for j in np.arange(glac126_2500.shape[1]):\n",
    "        if glac126_2500[i,j] > 294: glac126_2500[i,j] = 294\n",
    "glac126_2500data = np.hstack([years,glac126_2500/10])\n",
    "\n",
    "for i in np.arange(glac245_2500.shape[0]):\n",
    "    for j in np.arange(glac245_2500.shape[1]):\n",
    "        if glac245_2500[i,j] > 294: glac245_2500[i,j] = 294\n",
    "glac245_2500data = np.hstack([years,glac245_2500/10])\n",
    "\n",
    "glac26 = pd.DataFrame(np.vstack((quant,glac126_2500data[199,1:])).T)\n",
    "\n",
    "glac45 = pd.DataFrame(np.vstack((quant,glac245_2500data[199,1:])).T)\n",
    "\n",
    "\n",
    "glac_years = np.append(yearsnew,years)\n",
    "glac_126 = np.concatenate((annual126,glac126_2500))\n",
    "glac_245 = np.concatenate((annual245, glac245_2500))\n",
    "\n",
    "glac_126_csv = np.concatenate((np.reshape(glac_years, (481,1)), glac_126/1000),axis=1)\n",
    "glac_245_csv = np.concatenate((np.reshape(glac_years, (481,1)), glac_245/1000),axis=1)\n",
    "\n",
    "#np.savetxt('./Created_Data/Glaciers_2500_SSP126_quantiles.csv', glac_126_csv, delimiter=',')\n",
    "#np.savetxt('./Created_Data/Glaciers_2500_SSP245_quantiles.csv', glac_245_csv, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up all fast track quantiles for LWS, and change baseline to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4_CLASSIC data model, file format HDF5):\n",
      "    title: Global mean sea level projections up to 2500\n",
      "    subtitle: Including individual components and their combination (sum)\n",
      "    dimensions(sizes): quant(107), years(481), rcp(2)\n",
      "    variables(dimensions): float64 quant(quant), float64 years(years), float64 rcp(rcp), float64 the(quant, years, rcp), float64 lws(quant, years, rcp), float64 ant(quant, years, rcp), float64 gre(quant, years, rcp), float64 gic(quant, years, rcp), float64 tslc(quant, years, rcp)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "path = './Data/GMSL2500_fasttrack.nc'\n",
    "\n",
    "ds = nc.Dataset(path)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lws26 = ds['lws'][:,:,0]\n",
    "lws26 = lws26 - lws26[:,0:1]\n",
    "lws45 = ds['lws'][:,:,1]\n",
    "lws45 = lws45 - lws45[:,0:1]\n",
    "\n",
    "quant = ds['quant'][:]\n",
    "\n",
    "years_fast = ds['years'][:]\n",
    "\n",
    "lws26 = lws26.T\n",
    "lws45 = lws45.T\n",
    "\n",
    "lws_26_csv = np.concatenate((np.reshape(years_fast, (481,1)), lws26/100),axis=1)\n",
    "lws_45_csv = np.concatenate((np.reshape(years_fast, (481,1)), lws45/100),axis=1)\n",
    "\n",
    "#np.savetxt('./Created_Data/LWS_2500_RCP26_quantiles.csv', lws_26_csv, delimiter=',')\n",
    "#np.savetxt('./Created_Data/LWS_2500_RCP45_quantiles.csv', lws_45_csv, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up THE data from Victor. Change baseline to 2020, interpolate to annual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "the26 = pd.read_csv('./Data/TE26.csv', delimiter=';', header = None).T\n",
    "the45 = pd.read_csv('./Data/TE45.csv', delimiter=';', header = None).T\n",
    "\n",
    "the_years = np.asarray(pd.read_csv('./Data/ts_te.csv', delimiter=';', header = None))\n",
    "the_years = the_years.reshape((49,))\n",
    "\n",
    "the26 = the26 - the26.iloc[0,:]\n",
    "the45 = the45 - the45.iloc[0,:]\n",
    "\n",
    "annual_the26 = np.zeros((glac_years.shape[0], the26.shape[1]))\n",
    "for i in np.arange(the26.shape[1]):\n",
    "    f26 = interp1d(the_years, the26.iloc[:,i], fill_value=\"extrapolate\")\n",
    "    annual_the26[:,i] = f26(glac_years)\n",
    "    \n",
    "annual_the45 = np.zeros((glac_years.shape[0], the45.shape[1]))\n",
    "for i in np.arange(the45.shape[1]):\n",
    "    f45 = interp1d(the_years, the45.iloc[:,i], fill_value=\"extrapolate\")\n",
    "    annual_the45[:,i] = f45(glac_years)\n",
    "    \n",
    "the_26_csv = np.concatenate((np.reshape(glac_years, (481,1)), annual_the26),axis=1)\n",
    "the_45_csv = np.concatenate((np.reshape(glac_years, (481,1)), annual_the45),axis=1)\n",
    "\n",
    "np.savetxt('./Created_Data/THE_2500_RCP26_quantiles.csv', the_26_csv, delimiter=',')\n",
    "np.savetxt('./Created_Data/THE_2500_RCP45_quantiles.csv', the_45_csv, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all data in simulations into quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning:Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n"
     ]
    }
   ],
   "source": [
    "DeConto_26_quant = np.quantile(DeConto_26_2500[20:,:], quant, axis=1).T\n",
    "DeConto_45_quant = np.quantile(DeConto_45_2500[20:,:], quant, axis=1).T\n",
    "\n",
    "#np.savetxt('./Created_Data/DeConto_2500_RCP26_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), DeConto_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/DeConto_2500_RCP45_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), DeConto_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "Bulthuis_26_quant = np.quantile(annual_bult_26[20:,:], quant, axis=1).T\n",
    "Bulthuis_45_quant = np.quantile(annual_bult_45[20:,:], quant, axis=1).T\n",
    "\n",
    "#np.savetxt('./Created_Data/Bulthuis_2500_RCP26_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), Bulthuis_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/Bulthuis_2500_RCP45_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), Bulthuis_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "Lowry_26_quant = np.quantile(Lowry_26_2500[50:,:], quant, axis=1).T\n",
    "Lowry_45_quant = np.quantile(Lowry_45_2500[50:,:], quant, axis=1).T\n",
    "\n",
    "#np.savetxt('./Created_Data/Lowry_2500_RCP26_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), Lowry_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/Lowry_2500_RCP45_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), Lowry_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "Aschwanden_26_quant = np.quantile(SLE26[12:493,1:], quants, axis=1).T\n",
    "Aschwanden_45_quant = np.quantile(SLE45[12:493,1:], quants, axis=1).T\n",
    "\n",
    "#np.savetxt('./Created_Data/Aschwanden_2500_RCP26_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)),Aschwanden_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/Aschwanden_2500_RCP45_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)),Aschwanden_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "LARMIP_26_quant = np.quantile(LAR_126, quants, axis=1).T\n",
    "LARMIP_45_quant = np.quantile(LAR_245, quants, axis=1).T\n",
    "\n",
    "#np.savetxt('./Created_Data/LARMIP_2500_SSP126_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)),LARMIP_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/LARMIP_2500_SSP245_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)),LARMIP_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "GrIS_AR6_26_quant = np.quantile(gris_26/1000, quants, axis=1).T\n",
    "GrIS_AR6_45_quant = np.quantile(gris_45/1000, quants, axis=1).T\n",
    "\n",
    "#np.savetxt('./Created_Data/GrIS_AR6_2500_SSP126_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)),GrIS_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/GrIS_AR6_2500_SSP245_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)),GrIS_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "Glac_26_quant = glac_126_csv[:,1:]\n",
    "Glac_45_quant = glac_245_csv[:,1:]\n",
    "\n",
    "THE_26_quant = the_26_csv[:,1:]\n",
    "THE_45_quant = the_45_csv[:,1:]\n",
    "\n",
    "LWS_26_quant = lws_26_csv[:,1:]\n",
    "LWS_45_quant = lws_45_csv[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average AIS and GrIS data, and create GMSL quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrIS_26_quant = np.mean((Aschwanden_26_quant, GrIS_AR6_26_quant), axis = 0)\n",
    "GrIS_45_quant = np.mean((Aschwanden_45_quant, GrIS_AR6_45_quant), axis = 0)\n",
    "\n",
    "#np.savetxt('./Created_Data/GrIS_2500_SSP126_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), GrIS_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/GrIS_2500_SSP245_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), GrIS_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "AIS_26_quant = np.mean((DeConto_26_quant, Bulthuis_26_quant, Lowry_26_quant, LARMIP_26_quant), axis = 0)\n",
    "AIS_45_quant = np.mean((DeConto_45_quant, Bulthuis_45_quant, Lowry_45_quant, LARMIP_45_quant), axis = 0)\n",
    "\n",
    "#np.savetxt('./Created_Data/AIS_2500_RCP26_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), AIS_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/AIS_2500_RCP45_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), AIS_45_quant),axis=1), delimiter=',')\n",
    "\n",
    "GMSL_26_quant = AIS_26_quant + GrIS_26_quant + Glac_26_quant + THE_26_quant + LWS_26_quant\n",
    "GMSL_45_quant = AIS_45_quant + GrIS_45_quant + Glac_45_quant + THE_45_quant + LWS_45_quant\n",
    "\n",
    "#np.savetxt('./Created_Data/GMSL_2500_RCP26_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), GMSL_26_quant),axis=1), delimiter=',')\n",
    "#np.savetxt('./Created_Data/GMSL_2500_RCP45_quantiles.csv', np.concatenate((np.reshape(years_fast, (481,1)), GMSL_45_quant),axis=1), delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
